{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing the Fine-Tuned Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import time\n",
    "import torch\n",
    "\n",
    "import os\n",
    "import torch\n",
    "from typing import List, Dict, Any\n",
    "from datasets import load_dataset, load_from_disk, Dataset\n",
    "from huggingface_hub import login\n",
    "from transformers import (\n",
    "    AutoProcessor,\n",
    "    LlavaOnevisionForConditionalGeneration,\n",
    "    BitsAndBytesConfig,\n",
    ")\n",
    "from peft import LoraConfig, prepare_model_for_kbit_training, get_peft_model\n",
    "from trl import SFTConfig, SFTTrainer\n",
    "from PIL import Image\n",
    "import io\n",
    "\n",
    "from transformers import Qwen2VLProcessor\n",
    "from qwen_vl_utils import process_vision_info\n",
    "\n",
    "def clear_memory():\n",
    "    # Delete variables if they exist in the current global scope\n",
    "    if \"inputs\" in globals():\n",
    "        del globals()[\"inputs\"]\n",
    "    if \"model\" in globals():\n",
    "        del globals()[\"model\"]\n",
    "    if \"processor\" in globals():\n",
    "        del globals()[\"processor\"]\n",
    "    if \"trainer\" in globals():\n",
    "        del globals()[\"trainer\"]\n",
    "    if \"peft_model\" in globals():\n",
    "        del globals()[\"peft_model\"]\n",
    "    if \"bnb_config\" in globals():\n",
    "        del globals()[\"bnb_config\"]\n",
    "    time.sleep(2)\n",
    "\n",
    "    # Garbage collection and clearing CUDA memory\n",
    "    gc.collect()\n",
    "    time.sleep(2)\n",
    "    torch.cuda.empty_cache()\n",
    "    torch.cuda.synchronize()\n",
    "    time.sleep(2)\n",
    "    gc.collect()\n",
    "    time.sleep(2)\n",
    "\n",
    "    print(f\"GPU allocated memory: {torch.cuda.memory_allocated() / 1024**3:.2f} GB\")\n",
    "    print(f\"GPU reserved memory: {torch.cuda.memory_reserved() / 1024**3:.2f} GB\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU allocated memory: 0.01 GB\n",
      "GPU reserved memory: 0.02 GB\n"
     ]
    }
   ],
   "source": [
    "clear_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:10<00:00,  2.52s/it]\n"
     ]
    }
   ],
   "source": [
    "# Configuration\n",
    "MODEL_ID = \"llava-hf/llava-onevision-qwen2-7b-ov-hf\"\n",
    "# DATASET_ID = \"philschmid/amazon-product-descriptions-vlm\"\n",
    "USE_LORA = True\n",
    "USE_QLORA = False\n",
    "MULTIPLE_IMAGES_NUM = 2\n",
    "\n",
    "\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(MODEL_ID)\n",
    "\n",
    "if USE_QLORA or USE_LORA:\n",
    "    bnb_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_quant_type=\"nf4\",\n",
    "        bnb_4bit_compute_dtype=torch.bfloat16, # torch.float16\n",
    "    ) if USE_QLORA else None\n",
    "    \n",
    "    model = LlavaOnevisionForConditionalGeneration.from_pretrained(\n",
    "        MODEL_ID,\n",
    "        device_map=\"auto\",\n",
    "        torch_dtype=torch.bfloat16, # torch.float16,\n",
    "        quantization_config=bnb_config,\n",
    "    )\n",
    "else:\n",
    "    model = LlavaOnevisionForConditionalGeneration.from_pretrained(\n",
    "        MODEL_ID,\n",
    "        device_map=\"auto\",\n",
    "        torch_dtype=torch.bfloat16, # torch.float16,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "adapter_path = \"llava-onevision-qwen2-7b-ov-neurips-openreview-v1\"\n",
    "model.load_adapter(adapter_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_from_disk(\"neurips_openreview_v1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_data(sample):\n",
    "    return {\n",
    "        # \"images\": [Image.open(io.BytesIO(img)) for img in sample[\"image\"]],\n",
    "        \"messages\": [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": [{\"type\": \"text\", \"text\": \"You are a professional academic paper review assistant.\"}],\n",
    "        },\n",
    "        {   \n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                        # *[{'type': 'image', 'image': Image.open(io.BytesIO(img))} for img in sample[\"image\"]],\n",
    "                        *[{'type': 'image', 'image': Image.open(io.BytesIO(sample[\"image\"][i])).resize((336, 336))} for i in range(MULTIPLE_IMAGES_NUM)],\n",
    "                        # {'type': 'image', 'image': Image.open(io.BytesIO(sample[\"image\"][0]))},\n",
    "                        {\"type\": \"text\", \"text\": \"Please help me on reviewing this paper by given those images\"}\n",
    "                        ],\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"assistant\",\n",
    "            \"content\": [{\"type\": \"text\", \"text\": sample[\"summaries\"][0]}],\n",
    "        },\n",
    "    ]\n",
    "    }\n",
    "\n",
    "dataset = load_from_disk(\"neurips_openreview_v1\")\n",
    "format_dataset = [format_data(sample) for sample in dataset]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text_from_sample(model, processor, sample, max_new_tokens=1024, device=\"cuda\"):\n",
    "    # Prepare the text input by applying the chat template\n",
    "\n",
    "    # print(sample[\"messages\"][1:2])\n",
    "    text_input = processor.apply_chat_template(\n",
    "        sample[\"messages\"][1:2], tokenize=False, add_generation_prompt=True  # Use the sample without the system message\n",
    "    )\n",
    "\n",
    "    # Process the visual input from the sample\n",
    "    image_inputs, _ = process_vision_info(sample[\"messages\"])\n",
    "    \n",
    "    # Prepare the inputs for the model\n",
    "    model_inputs = processor(\n",
    "        text=[text_input],\n",
    "        images=image_inputs,\n",
    "        return_tensors=\"pt\",\n",
    "    ).to(\n",
    "        device, dtype=torch.bfloat16\n",
    "    ) # Move inputs to the specified device\n",
    "\n",
    "    # Generate text with the model\n",
    "    generated_ids = model.generate(**model_inputs, max_new_tokens=max_new_tokens)\n",
    "\n",
    "    # Trim the generated ids to remove the input ids\n",
    "    trimmed_generated_ids = [out_ids[len(in_ids) :] for in_ids, out_ids in zip(model_inputs.input_ids, generated_ids)]\n",
    "\n",
    "    # Decode the output text\n",
    "    output_text = processor.batch_decode(\n",
    "        trimmed_generated_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False\n",
    "    )\n",
    "\n",
    "    return output_text[0]  # Return the first decoded output text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'messages': [{'role': 'system', 'content': [{'type': 'text', 'text': 'You are a professional academic paper review assistant.'}]}, {'role': 'user', 'content': [{'type': 'image', 'image': <PIL.Image.Image image mode=RGB size=336x336 at 0x7F837EE8BF40>}, {'type': 'image', 'image': <PIL.Image.Image image mode=RGB size=336x336 at 0x7F837EE8B9A0>}, {'type': 'text', 'text': 'Please help me on reviewing this paper by given those images'}]}, {'role': 'assistant', 'content': [{'type': 'text', 'text': 'The authors take the generalization of the K-NN method for the multi-label classification problem, which lifts the samples to feature spaces and replaces the distance weights with more general weight functions. The distributionally robust formulation of this well-known generalization is defined and shown to be equivalent to a much simpler problem when the ambiguity sets comprise Wasserstein balls. Thanks to this equivalence, the authors show that the worst-case distributions are characterized by the solution of a convex optimization problem. There is further a solution algorithm proposed, and thanks to this, the authors compare the performance of Wasserstein DRO weighted K-NN with benchmark algorithms on well-known classification datasets.'}]}]}\n"
     ]
    }
   ],
   "source": [
    "print(format_dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "output = generate_text_from_sample(model, processor, format_dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The paper proposes a method for multi-class classification with only 3 labeled training samples. The method is based on the weighted k-NN algorithm, where the weights are learned. The authors show that the proposed method is equivalent to a SVM with a particular feature mapping. They also show that the feature mapping can be computed in linear time, which allows for efficient training. The authors show that the method is able to classify the 3 samples correctly on the training set, and show some experiments on real-world datasets.\n"
     ]
    }
   ],
   "source": [
    "# Prediction\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ground-Truth Result "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The authors take the generalization of the K-NN method for the multi-label classification problem, which lifts the samples to feature spaces and replaces the distance weights with more general weight functions. The distributionally robust formulation of this well-known generalization is defined and shown to be equivalent to a much simpler problem when the ambiguity sets comprise Wasserstein balls. Thanks to this equivalence, the authors show that the worst-case distributions are characterized by the solution of a convex optimization problem. There is further a solution algorithm proposed, and thanks to this, the authors compare the performance of Wasserstein DRO weighted K-NN with benchmark algorithms on well-known classification datasets.\n"
     ]
    }
   ],
   "source": [
    "# GT \n",
    "print(format_dataset[0][\"messages\"][2:3][0][\"content\"][0]['text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare Fine-Tuned Model vs. Base Model + Prompting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:08<00:00,  2.07s/it]\n"
     ]
    }
   ],
   "source": [
    "# Configuration\n",
    "MODEL_ID = \"llava-hf/llava-onevision-qwen2-7b-ov-hf\"\n",
    "# DATASET_ID = \"philschmid/amazon-product-descriptions-vlm\"\n",
    "USE_LORA = True\n",
    "USE_QLORA = False\n",
    "MULTIPLE_IMAGES_NUM = 2\n",
    "\n",
    "\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(MODEL_ID)\n",
    "\n",
    "if USE_QLORA or USE_LORA:\n",
    "    bnb_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_quant_type=\"nf4\",\n",
    "        bnb_4bit_compute_dtype=torch.bfloat16, # torch.float16\n",
    "    ) if USE_QLORA else None\n",
    "    \n",
    "    model = LlavaOnevisionForConditionalGeneration.from_pretrained(\n",
    "        MODEL_ID,\n",
    "        device_map=\"auto\",\n",
    "        torch_dtype=torch.bfloat16, # torch.float16,\n",
    "        quantization_config=bnb_config,\n",
    "    )\n",
    "else:\n",
    "    model = LlavaOnevisionForConditionalGeneration.from_pretrained(\n",
    "        MODEL_ID,\n",
    "        device_map=\"auto\",\n",
    "        torch_dtype=torch.bfloat16, # torch.float16,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The image you've provided appears to be a page from a research paper. However, I'm unable to provide a detailed review of the content or the quality of the research based on the images alone. If you have specific questions about the paper or need assistance with understanding certain aspects of the content, feel free to ask, and I'll do my best to help!\n"
     ]
    }
   ],
   "source": [
    "output = generate_text_from_sample(model, processor, format_dataset[0])\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ftvlm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
