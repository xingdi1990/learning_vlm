{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. set up the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://huggingface.co/docs/transformers/en/model_doc/llava\n",
    "\n",
    "\n",
    "from transformers import AutoProcessor, AutoTokenizer, LlavaOnevisionForConditionalGeneration\n",
    "import torch\n",
    "import os\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "\n",
    "model_id = \"llava-hf/llava-onevision-qwen2-7b-ov-hf\"\n",
    "processor = AutoProcessor.from_pretrained(model_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Create and prepare the dataset\n",
    "\n",
    "Once you have determined that fine-tuning is the right solution we need to create a dataset to fine-tune our model. We have to prepare the dataset in a format that the model can understand.\n",
    "\n",
    "TRL supports popular instruction and conversation dataset formats. This means we only need to convert our dataset to one of the supported formats and trl will take care of the rest.\n",
    "```\n",
    "\"messages\": [\n",
    "  {\"role\": \"system\", \"content\": [{\"type\":\"text\", \"text\": \"You are a helpful....\"}]},\n",
    "  {\"role\": \"user\", \"content\": [{\n",
    "    \"type\": \"text\", \"text\":  \"How many dogs are in the image?\", \n",
    "    \"type\": \"image\", \"text\": <PIL.Image> \n",
    "    }]},\n",
    "  {\"role\": \"assistant\", \"content\": [{\"type\":\"text\", \"text\": \"There are 3 dogs in the image.\"}]}\n",
    "],\n",
    "```\n",
    "In our example we are going to load our dataset using the Datasets library and apply our frompt and convert it into the the conversational format.\n",
    "\n",
    "Lets start!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import io\n",
    "\n",
    "def format_data(sample):\n",
    "    return {\n",
    "        # \"images\": [Image.open(io.BytesIO(img)) for img in sample[\"image\"]],\n",
    "        \"messages\": [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": [{\"type\": \"text\", \"text\": \"You are a professional academic paper review assistant.\"}],\n",
    "        },\n",
    "        {   \n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                        *[{'type': 'image', 'image': Image.open(io.BytesIO(img))} for img in sample[\"image\"]],\n",
    "                        # {'type': 'image'},\n",
    "                        {\"type\": \"text\", \"text\": \"Please help me on reviewing this paper by given those images\"}\n",
    "                        ],\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"assistant\",\n",
    "            \"content\": [{\"type\": \"text\", \"text\": sample[\"summaries\"][0]}],\n",
    "        },\n",
    "    ]\n",
    "    }\n",
    "\n",
    "\n",
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"DetionDX/neurips_openreview_v1\", split=\"train\")\n",
    "\n",
    "print(dataset)\n",
    "\n",
    "def display_example(example):\n",
    "    print(f\"ID: {example['id']}\")\n",
    "    print(f\"Page numbers: {example['page_number']}\")\n",
    "    print(f\"Number of images: {len(example['image'])}\")\n",
    "    print(f\"Number of summaries: {len(example['summaries'])}\")\n",
    "    print(f\"First summary: {example['summaries'][0]}\")\n",
    "    print(\"First image:\")\n",
    "    \n",
    "    # Convert bytes to PIL Image\n",
    "    image_bytes = example['image'][0]\n",
    "    image = Image.open(io.BytesIO(image_bytes))\n",
    "    \n",
    "    # Display using matplotlib\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    plt.imshow(image)\n",
    "    plt.axis('off')  # Hide axes\n",
    "    plt.show()\n",
    "\n",
    "# Display an example\n",
    "display_example(dataset[0])\n",
    "\n",
    "dataset = [format_data(sample) for sample in dataset]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Fine-tune VLM using trl and the SFTTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://huggingface.co/docs/transformers/en/model_doc/llava\n",
    "\n",
    "from transformers import AutoProcessor, LlavaOnevisionForConditionalGeneration, BitsAndBytesConfig\n",
    "import torch\n",
    "\n",
    "model_id = \"llava-hf/llava-onevision-qwen2-7b-ov-hf\"\n",
    "from transformers import BitsAndBytesConfig\n",
    "\n",
    "from peft import LoraConfig, prepare_model_for_kbit_training, get_peft_model\n",
    "\n",
    "\n",
    "\n",
    "USE_LORA = True\n",
    "USE_QLORA = False\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True, bnb_4bit_quant_type=\"nf4\", bnb_4bit_compute_dtype=torch.float16\n",
    ")\n",
    "\n",
    "model = LlavaOnevisionForConditionalGeneration.from_pretrained(\n",
    "    model_id,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    quantization_config=bnb_config if USE_QLORA else None,\n",
    ")\n",
    "\n",
    "def find_all_linear_names(model):\n",
    "    cls = torch.nn.Linear\n",
    "    lora_module_names = set()\n",
    "    multimodal_keywords = ['multi_modal_projector', 'vision_model']\n",
    "    for name, module in model.named_modules():\n",
    "        if any(mm_keyword in name for mm_keyword in multimodal_keywords):\n",
    "            continue\n",
    "        if isinstance(module, cls):\n",
    "            names = name.split('.')\n",
    "            lora_module_names.add(names[0] if len(names) == 1 else names[-1])\n",
    "\n",
    "    if 'lm_head' in lora_module_names: # needed for 16-bit\n",
    "        lora_module_names.remove('lm_head')\n",
    "    return list(lora_module_names)\n",
    "\n",
    "if USE_LORA:\n",
    "\n",
    "    lora_config = LoraConfig(\n",
    "        r=8,\n",
    "        lora_alpha=8,\n",
    "        lora_dropout=0.05,\n",
    "        target_modules=find_all_linear_names(model),\n",
    "        init_lora_weights=\"gaussian\",\n",
    "    )\n",
    "    if USE_QLORA:\n",
    "        model = prepare_model_for_kbit_training(model)\n",
    "        \n",
    "    model = get_peft_model(model, lora_config)\n",
    "\n",
    "\n",
    "\n",
    "    model.print_trainable_parameters()\n",
    "\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a data collator to encode text and image pairs\n",
    "from transformers import Qwen2VLProcessor\n",
    "from qwen_vl_utils import process_vision_info\n",
    "\n",
    "def collate_fn(examples):\n",
    "    # Get the texts and images, and apply the chat template\n",
    "    texts = [processor.apply_chat_template(example[\"messages\"], tokenize=False) for example in examples]\n",
    "    # print(texts)\n",
    "    # print(examples[0][\"messages\"])\n",
    "    image_inputs = [process_vision_info(example[\"messages\"])[0] for example in examples]\n",
    "\n",
    "    \n",
    "    # Tokenize the texts and process the images\n",
    "    batch = processor(text=texts, images=image_inputs, return_tensors=\"pt\", padding=True)\n",
    "\n",
    "    # The labels are the input_ids, and we mask the padding tokens in the loss computation\n",
    "    labels = batch[\"input_ids\"].clone()\n",
    "    labels[labels == processor.tokenizer.pad_token_id] = -100  #\n",
    "    # Ignore the image token index in the loss computation (model specific)\n",
    "    if isinstance(processor, Qwen2VLProcessor):\n",
    "        image_tokens = [151652,151653,151655]\n",
    "    else: \n",
    "        image_tokens = [processor.tokenizer.convert_tokens_to_ids(processor.image_token)]\n",
    "    for image_token_id in image_tokens:\n",
    "        labels[labels == image_token_id] = -100\n",
    "    batch[\"labels\"] = labels\n",
    "\n",
    "    return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "collate_fn([dataset[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trl import (\n",
    "    ModelConfig,\n",
    "    SFTConfig,\n",
    "    SFTTrainer,\n",
    "    TrlParser,\n",
    "    get_kbit_device_map,\n",
    "    get_peft_config,\n",
    "    get_quantization_config,\n",
    ")\n",
    "\n",
    "# from transformers import TrainingArguments\n",
    "\n",
    "args = SFTConfig(\n",
    "    output_dir=\"llava-onevision-qwen2-7b-ov-neurips-openreview-v1\", # directory to save and repository id\n",
    "    num_train_epochs=10,                     # number of training epochs\n",
    "    per_device_train_batch_size=1,          # batch size per device during training\n",
    "    gradient_accumulation_steps=8,          # number of steps before performing a backward/update pass\n",
    "    gradient_checkpointing=True,            # use gradient checkpointing to save memory\n",
    "    optim=\"adamw_hf\",              # use fused adamw optimizer\n",
    "    logging_steps=5,                       # log every 10 steps\n",
    "    save_strategy=\"epoch\",                  # save checkpoint every epoch\n",
    "    learning_rate=2e-4,                     # learning rate, based on QLoRA paper\n",
    "    # fp16=False,\n",
    "    bf16=True,                              # use bfloat16 precision\n",
    "    tf32=True,                              # use tf32 precision\n",
    "    max_grad_norm=0.3,                      # max gradient norm based on QLoRA paper\n",
    "    warmup_ratio=0.1,                      # warmup ratio based on QLoRA paper\n",
    "    lr_scheduler_type=\"constant\",           # use constant learning rate scheduler\n",
    "    push_to_hub=False,                       # push model to hub\n",
    "    report_to=\"tensorboard\",                # report metrics to tensorboard\n",
    "    gradient_checkpointing_kwargs = {\"use_reentrant\": False}, # use reentrant checkpointing\n",
    "    dataset_text_field=\"\", # need a dummy field for collator\n",
    "    dataset_kwargs = {\"skip_prepare_dataset\": True} # important for collator\n",
    ")\n",
    "args.remove_unused_columns=False\n",
    "\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=dataset,\n",
    "    data_collator=collate_fn,\n",
    "    dataset_text_field=\"\", # needs dummy value\n",
    "    peft_config=lora_config,\n",
    "    tokenizer=processor.tokenizer,\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start training, the model will be automatically saved to the hub and the output directory\n",
    "trainer.train()\n",
    "\n",
    "# save model \n",
    "trainer.save_model(args.output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# free the memory again\n",
    "del model\n",
    "del trainer\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llamafactory",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
