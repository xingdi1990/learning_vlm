{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to first load the dataset, the processor and the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "\n",
    "login(\"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\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://huggingface.co/docs/transformers/en/model_doc/llava\n",
    "\n",
    "from transformers import AutoProcessor, AutoTokenizer, MllamaForConditionalGeneration\n",
    "import torch\n",
    "\n",
    "import os\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"2\"\n",
    "\n",
    "\n",
    "model_id = \"meta-llama/Llama-3.2-11B-Vision-Instruct\"\n",
    "processor = AutoProcessor.from_pretrained(model_id)\n",
    "print(processor)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "print(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# model = MllamaForConditionalGeneration.from_pretrained(\n",
    "#     model_id,\n",
    "#     torch_dtype=torch.float16,\n",
    "#     low_cpu_mem_usage=True\n",
    "# )\n",
    "# model.to(device)\n",
    "\n",
    "\n",
    "# Unlike direct load the pretrained model, we choose the PEFT strategy for finetuning with Lora.\n",
    "# https://huggingface.co/docs/peft/en/index\n",
    "# \n",
    "\n",
    "from transformers import BitsAndBytesConfig\n",
    "\n",
    "from peft import LoraConfig, prepare_model_for_kbit_training, get_peft_model\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "USE_LORA = True\n",
    "USE_QLORA = True\n",
    "\n",
    "## Load model\n",
    "\n",
    "# Three options for training, from the lowest precision training to the highest precision training:\n",
    "# - QLora\n",
    "# - Standard Lora\n",
    "# - Full fine-tuning\n",
    "if USE_QLORA or USE_LORA:\n",
    "    if USE_QLORA:\n",
    "        bnb_config = BitsAndBytesConfig(\n",
    "            load_in_4bit=True, bnb_4bit_quant_type=\"nf4\", bnb_4bit_compute_dtype=torch.float16\n",
    "        )\n",
    "    model = MllamaForConditionalGeneration.from_pretrained(\n",
    "        model_id,\n",
    "        torch_dtype=torch.float16,\n",
    "        quantization_config=bnb_config if USE_QLORA else None,\n",
    "        low_cpu_mem_usage=True\n",
    "    )\n",
    "else:\n",
    "    # for full fine-tuning, we can speed up the model using Flash Attention\n",
    "    # only available on certain devices, see https://github.com/Dao-AILab/flash-attention?tab=readme-ov-file#installation-and-features\n",
    "    model = MllamaForConditionalGeneration.from_pretrained(\n",
    "        model_id,\n",
    "        torch_dtype=torch.float16,\n",
    "        low_cpu_mem_usage=True\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def find_all_linear_names(model):\n",
    "    cls = torch.nn.Linear\n",
    "    lora_module_names = set()\n",
    "    multimodal_keywords = ['multi_modal_projector', 'vision_model']\n",
    "    for name, module in model.named_modules():\n",
    "        if any(mm_keyword in name for mm_keyword in multimodal_keywords):\n",
    "            continue\n",
    "        if isinstance(module, cls):\n",
    "            names = name.split('.')\n",
    "            lora_module_names.add(names[0] if len(names) == 1 else names[-1])\n",
    "\n",
    "    if 'lm_head' in lora_module_names: # needed for 16-bit\n",
    "        lora_module_names.remove('lm_head')\n",
    "    return list(lora_module_names)\n",
    "\n",
    "if USE_LORA:\n",
    "\n",
    "    lora_config = LoraConfig(\n",
    "        r=8,\n",
    "        lora_alpha=8,\n",
    "        lora_dropout=0.1,\n",
    "        target_modules=find_all_linear_names(model),\n",
    "        init_lora_weights=\"gaussian\",\n",
    "    )\n",
    "    if USE_QLORA:\n",
    "        model = prepare_model_for_kbit_training(model)\n",
    "    model = get_peft_model(model, lora_config)\n",
    "\n",
    "\n",
    "\n",
    "    model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset_id = \"philschmid/amazon-product-descriptions-vlm\"\n",
    "dataset = load_dataset(dataset_id, split=\"train\")\n",
    "print(dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example = dataset[0]\n",
    "print(example)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example[\"image\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example[\"description\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example[\"Product Name\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  you need to convert the dataset format to the the TRL trainer \n",
    "#  https://huggingface.co/docs/trl/en/sft_trainer\n",
    "# https://blog.futuresmart.ai/fine-tune-llama-32-vision-language-model-on-custom-datasets\n",
    "# https://huggingface.co/meta-llama/Llama-3.2-11B-Vision-Instruct/discussions/31\n",
    "\n",
    "prompt = \"\"\"Create a Short Product description based on the provided ##PRODUCT NAME## and ##CATEGORY## and image.\n",
    "Only return description. The description should be SEO optimized and for a better mobile search experience.\n",
    "\n",
    "##PRODUCT NAME##: {product_name}\n",
    "##CATEGORY##: {category}\"\"\"\n",
    "\n",
    "\n",
    "def format_data(sample):\n",
    "    return {\n",
    "    \"messages\": [\n",
    "        {'content': [{'text': prompt.format(product_name=sample[\"Product Name\"], category=sample[\"Category\"]),\n",
    "                        'type': 'text'},\n",
    "                        {'text': None, 'type': 'image'}],\n",
    "                        'role': 'user'},\n",
    "        {'content': [{'text': sample[\"description\"], 'type': 'text'}],\n",
    "            'role': 'assistant'},\n",
    "    ],\n",
    "    \"images\": [sample[\"image\"]],\n",
    "    }\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "formatted_dataset = [format_data(sample) for sample in dataset]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from qwen_vl_utils import process_vision_info\n",
    "\n",
    "class DataCollator:\n",
    "    def __init__(self, processor):\n",
    "        self.processor = processor\n",
    "    ################\n",
    "    # Create a data collator to encode text and image pairs\n",
    "    ################\n",
    "    def __call__(self, examples):\n",
    "        texts = []\n",
    "        images = []\n",
    "        for example in examples:\n",
    "            messages = example[\"messages\"]\n",
    "            text = self.processor.apply_chat_template(messages, tokenize=False)\n",
    "            texts.append(text)\n",
    "            # images.append(process_vision_info(example[\"messages\"])[0])\n",
    "            images.append(example[\"images\"][0])\n",
    "\n",
    "\n",
    "        # print(texts)\n",
    "        batch = self.processor(text=texts, images=images, return_tensors=\"pt\", padding=True)\n",
    "\n",
    "        # The labels are the input_ids, and we mask the padding tokens in the loss computation\n",
    "        labels = batch[\"input_ids\"].clone()\n",
    "        if self.processor.tokenizer.pad_token_id is not None:\n",
    "            labels[labels == self.processor.tokenizer.pad_token_id] = -100\n",
    "\n",
    "        # Ignore the image token index in the loss computation (model specific)\n",
    "        image_token_id = processor.tokenizer.convert_tokens_to_ids(processor.image_token)\n",
    "        labels[labels == image_token_id] = -100\n",
    "        batch[\"labels\"] = labels\n",
    "\n",
    "        return batch\n",
    "    \n",
    "\n",
    "data_collator = DataCollator(processor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(processor.apply_chat_template(dataset[0][\"messages\"], tokenize=False, add_generation_prompt=False))\n",
    "formatted_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(processor.chat_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator([formatted_dataset[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "standard_dataset = load_dataset(\"HuggingFaceH4/llava-instruct-mix-vsft\", split=\"train\")\n",
    "\n",
    "\n",
    "standard_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trl import (\n",
    "    ModelConfig,\n",
    "    SFTConfig,\n",
    "    SFTTrainer,\n",
    "    TrlParser,\n",
    "    get_kbit_device_map,\n",
    "    get_peft_config,\n",
    "    get_quantization_config,\n",
    ")\n",
    "\n",
    "from transformers import TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./llama3.2_vision_instruct_output\",\n",
    "    learning_rate= 1.4e-5,\n",
    "    per_device_train_batch_size=4,\n",
    "    num_train_epochs=2,\n",
    "    bf16=True,\n",
    "    remove_unused_columns=False,\n",
    "    push_to_hub=False\n",
    "    # gradient_checkpointing=False,\n",
    "    # gradient_accumulation_steps=8,\n",
    ")\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=formatted_dataset,\n",
    "    eval_dataset=None,\n",
    "    dataset_text_field=\"text\",  # need a dummy field\n",
    "    tokenizer=processor.tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    dataset_kwargs={\"skip_prepare_dataset\": True},\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save and push to hub\n",
    "trainer.save_model(training_args.output_dir)\n",
    "if training_args.push_to_hub:\n",
    "    trainer.push_to_hub()\n",
    "    if trainer.accelerator.is_main_process:\n",
    "        processor.push_to_hub(training_args.hub_model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llamafactory",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
