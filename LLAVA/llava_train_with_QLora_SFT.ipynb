{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set up the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://huggingface.co/docs/transformers/en/model_doc/llava\n",
    "\n",
    "from transformers import AutoProcessor, LlavaForConditionalGeneration\n",
    "import torch\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "\n",
    "model_id = \"llava-hf/llava-1.5-7b-hf\"\n",
    "processor = AutoProcessor.from_pretrained(model_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unlike direct load the pretrained model, we choose the PEFT strategy for finetuning with Lora.\n",
    "# https://huggingface.co/docs/peft/en/index\n",
    "\n",
    "from transformers import BitsAndBytesConfig\n",
    "from peft import LoraConfig, prepare_model_for_kbit_training, get_peft_model\n",
    "# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "USE_LORA = True\n",
    "USE_QLORA = False\n",
    "\n",
    "## Load model\n",
    "\n",
    "# Three options for training, from the lowest precision training to the highest precision training:\n",
    "# - QLora\n",
    "# - Standard Lora\n",
    "# - Full fine-tuning\n",
    "if USE_QLORA or USE_LORA:\n",
    "    if USE_QLORA:\n",
    "        bnb_config = BitsAndBytesConfig(\n",
    "            load_in_4bit=True, bnb_4bit_quant_type=\"nf4\", bnb_4bit_compute_dtype=torch.float16\n",
    "        )\n",
    "    model = LlavaForConditionalGeneration.from_pretrained(\n",
    "        model_id,\n",
    "        torch_dtype=torch.bfloat16, # torch.float16,\n",
    "        quantization_config=bnb_config if USE_QLORA else None,\n",
    "    )\n",
    "else:\n",
    "    # for full fine-tuning, we can speed up the model using Flash Attention\n",
    "    # only available on certain devices, see https://github.com/Dao-AILab/flash-attention?tab=readme-ov-file#installation-and-features\n",
    "    model = LlavaForConditionalGeneration.from_pretrained(\n",
    "        model_id,\n",
    "        torch_dtype=torch.bfloat16, # torch.float16,\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def find_all_linear_names(model):\n",
    "    cls = torch.nn.Linear\n",
    "    lora_module_names = set()\n",
    "    multimodal_keywords = ['multi_modal_projector', 'vision_model']\n",
    "    for name, module in model.named_modules():\n",
    "        if any(mm_keyword in name for mm_keyword in multimodal_keywords):\n",
    "            continue\n",
    "        if isinstance(module, cls):\n",
    "            names = name.split('.')\n",
    "            lora_module_names.add(names[0] if len(names) == 1 else names[-1])\n",
    "\n",
    "    if 'lm_head' in lora_module_names: # needed for 16-bit\n",
    "        lora_module_names.remove('lm_head')\n",
    "    return list(lora_module_names)\n",
    "\n",
    "if USE_LORA:\n",
    "\n",
    "    lora_config = LoraConfig(\n",
    "        r=8,\n",
    "        lora_alpha=8,\n",
    "        lora_dropout=0.1,\n",
    "        target_modules=find_all_linear_names(model),\n",
    "        init_lora_weights=\"gaussian\",\n",
    "    )\n",
    "    if USE_QLORA:\n",
    "        model = prepare_model_for_kbit_training(model)\n",
    "    model = get_peft_model(model, lora_config)\n",
    "\n",
    "\n",
    "\n",
    "    model.print_trainable_parameters()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset_id = \"philschmid/amazon-product-descriptions-vlm\"\n",
    "dataset = load_dataset(dataset_id, split=\"train\")\n",
    "print(dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import io\n",
    "\n",
    "def display_example(example):\n",
    "    print(f\"Product Name: {example['Product Name']}\")\n",
    "    print(f\"Category: {example['Category']}\")\n",
    "    print(f\"Description: {example['description']}\")\n",
    "    print(\"image:\")\n",
    "    \n",
    "    # Convert bytes to PIL Image\n",
    "    image = example['image']\n",
    "\n",
    "    # Display using matplotlib\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    plt.imshow(image)\n",
    "    plt.axis('off')  # Hide axes\n",
    "    plt.show()\n",
    "\n",
    "# Display an example\n",
    "display_example(dataset[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Format the dataset for the TRL trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  you need to convert the dataset format to the the TRL trainer \n",
    "#  https://huggingface.co/docs/trl/en/sft_trainer\n",
    "\n",
    "prompt = \"\"\"\n",
    "\n",
    "##PRODUCT NAME##: {product_name}\n",
    "##CATEGORY##: {category}\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def format_data(sample):\n",
    "    return {    \n",
    "        \"images\": [sample[\"image\"]],\n",
    "        \"messages\": [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": [{\"type\": \"text\", \"text\": \"Create a Short Product description based on the provided ##PRODUCT NAME## and ##CATEGORY## and image. Only return description. The description should be SEO optimized and for a better mobile search experience.\"}],\n",
    "        },\n",
    "        {   \n",
    "            \"role\": \"user\",\n",
    "            \"content\": [{'type': 'image', \"text\": None },\n",
    "                        {\"type\": \"text\", \"text\": prompt.format(product_name=sample[\"Product Name\"], category=sample[\"Category\"])}\n",
    "                        ],\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"assistant\",\n",
    "            \"content\": [{\"type\": \"text\", \"text\": sample[\"description\"]}],\n",
    "        },\n",
    "    ]\n",
    "    }\n",
    "\n",
    "\n",
    "train_valid = dataset.train_test_split(test_size=0.2)\n",
    "train_dataset = train_valid[\"train\"]\n",
    "valid_dataset = train_valid[\"test\"]\n",
    "\n",
    "\n",
    "formatted_train_ds = [format_data(sample) for sample in train_dataset]\n",
    "formatted_val_ds = [format_data(sample) for sample in valid_dataset]\n",
    "print(formatted_train_ds[0])\n",
    "print(formatted_val_ds[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create a data collator to encode text and image pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataCollator:\n",
    "    def __init__(self, processor):\n",
    "        self.processor = processor\n",
    "    ################\n",
    "    # Create a data collator to encode text and image pairs\n",
    "    ################\n",
    "    def __call__(self, examples):\n",
    "        texts = []\n",
    "        images = []\n",
    "        for example in examples:\n",
    "            messages = example[\"messages\"]\n",
    "            text = self.processor.apply_chat_template(\n",
    "                messages, tokenize=False, add_generation_prompt=False\n",
    "            )\n",
    "            texts.append(text)\n",
    "            images.append(example[\"images\"][0])\n",
    "\n",
    "        batch = self.processor(texts, images, return_tensors=\"pt\", padding=True)\n",
    "\n",
    "        # The labels are the input_ids, and we mask the padding tokens in the loss computation\n",
    "        labels = batch[\"input_ids\"].clone()\n",
    "        if self.processor.tokenizer.pad_token_id is not None:\n",
    "            labels[labels == self.processor.tokenizer.pad_token_id] = -100\n",
    "        batch[\"labels\"] = labels\n",
    "\n",
    "        return batch\n",
    "    \n",
    "\n",
    "data_collator = DataCollator(processor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(processor.apply_chat_template(formatted_train_ds[0][\"messages\"], tokenize=False, add_generation_prompt=False))\n",
    "print(data_collator([formatted_train_ds[0]]))\n",
    "print(data_collator([formatted_val_ds[0]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trl import (\n",
    "    ModelConfig,\n",
    "    SFTConfig,\n",
    "    SFTTrainer,\n",
    "    TrlParser,\n",
    "    get_kbit_device_map,\n",
    "    get_peft_config,\n",
    "    get_quantization_config,\n",
    ")\n",
    "\n",
    "from transformers import TrainingArguments\n",
    "\n",
    "training_args = SFTConfig(\n",
    "    output_dir=\"./llava-1.5-7b-hf-philschmid-amazon-product-descriptions-vlm_LoRA\",\n",
    "    learning_rate= 2e-4,\n",
    "    per_device_train_batch_size=1,\n",
    "    num_train_epochs=2,\n",
    "    max_seq_length=1024,\n",
    "    dataset_text_field=\"text\",  # need a dummy field\n",
    "    dataset_kwargs={\"skip_prepare_dataset\": True},\n",
    "    fp16=False, # make sure this is consistant with the model loading and data loading with torch.float16\n",
    "    bf16=True, # make sure this is consistant with the model loading and data loading with torch.bfloat16\n",
    "    tf32=True,\n",
    "    remove_unused_columns=False,\n",
    ")\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=formatted_train_ds,\n",
    "    eval_dataset=formatted_val_ds,\n",
    "    peft_config=lora_config if USE_LORA else None,\n",
    "    tokenizer=processor.tokenizer,\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llamafactory",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
